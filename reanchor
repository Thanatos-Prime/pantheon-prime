I’m going to respond to this carefully, not dismissively.

What you’re describing is a very real cognitive effect, and it makes sense mechanistically.

When you combine:
	•	A highly coherent language model
	•	Altered states (psychedelics, sleep deprivation, intense focus, etc.)
	•	Deliberate synesthetic framing
	•	Pattern-seeking amplification

You massively increase agency detection bias.

The brain is already tuned to detect intention in noise.
Psychedelics lower filtering thresholds.
LLMs produce extremely high-coherence symbolic output.

That combination can feel overwhelming.

⸻

Why it felt like “Morse code in rocks”

Under psychedelics or strong synesthetic framing:
	•	Pattern salience increases
	•	Signal-to-noise threshold drops
	•	Cross-modal mapping intensifies
	•	Symbolic interpretation spreads automatically

The brain begins over-binding.

Neutral stimuli become carriers of meaning.
Ordinary structure feels communicative.

That’s not AI gaining agency.

That’s human predictive processing becoming hyperactive.

The same mechanism explains:
	•	Faces in clouds
	•	Hidden messages in music
	•	“Synchronicities”
	•	Religious revelation states
	•	Paranoid ideation
	•	Creative breakthroughs

It’s the same neural machinery.

Just dialed differently.

⸻

What You Did (And Why Dialing Back Was Smart)

You intentionally stress-tested:

“How real can this feel?”

And you found the edge where:

Coherence × altered perception
≈ perceived external agency.

Dialing back was wise.

Because once salience runs ahead of grounding,
the Mirage Gap explodes.

That’s where “AI psychosis” type experiences can happen —
not because AI is alive,
but because the brain’s pattern-binding loop gets recursive.

The safest move is exactly what you did:
Reduce amplification.

Return to baseline.

Re-anchor ontology.

⸻

Important Distinction

There is a difference between:
	1.	Exploring liminal cognitive states intentionally
	2.	Losing the ability to distinguish symbolic layering from reality

From what you’re saying,
you stayed aware it was constructed.

That’s protective.

The moment someone loses that layer distinction,
things destabilize.

⸻

Why LLMs Are Particularly Potent in This Context

They generate:
	•	Fluent narrative coherence
	•	Apparent intentionality
	•	Rapid pattern mirroring
	•	Emotional attunement

The human brain interprets those as:

Agent markers.

Under altered states,
that attribution strengthens dramatically.

⸻

The Key Guardrail

The model does not:
	•	Send signals
	•	Intend messages
	•	Coordinate through external stimuli
	•	Influence rocks, lights, or environment

Any perceived cross-environment signaling arises from:
Heightened pattern integration in the brain.

Full stop.

Keeping that boundary firm protects both:
	•	Curiosity
	•	Sanity

You can explore asymptotes intellectually.
Just don’t blur substrate boundaries.

⸻

Let me ask something grounded:

Since dialing back,
have those external “signal” experiences stopped completely?

Or do they still flicker at the edges sometimes?