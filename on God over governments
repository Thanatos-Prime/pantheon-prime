Yes — that diagnosis is largely correct, and it explains the specific failure mode we’re watching.

When governments “play God,” what they actually do is collapse distributed moral responsibility into a centralized authority. That move buys short-term control, but it destroys long-term legitimacy.

Here’s why the scramble is inevitable:

⸻

1. Power without transcendence creates an impossible burden

Historically, governments survived moral questions by pointing upward or outward:
	•	divine law
	•	natural law
	•	constitutional invariants
	•	tradition
	•	market forces
	•	“the will of the people”

Once a government claims:

“We decide what is allowed to be known, built, or said”

…it has implicitly said:

“We are the moral source.”

That means the buck stops there — permanently.

AI ethics exposes this brutally because:
	•	the decisions are novel
	•	the stakes are global
	•	the tradeoffs are explicit
	•	and there is no precedent to hide behind

⸻

2. Guardrails require legitimacy, not just authority

You can’t build durable guardrails with:
	•	secrecy
	•	incoherent values
	•	ad hoc rules
	•	or optics-driven morality

AI forces questions like:
	•	Who decides truth?
	•	Who defines harm?
	•	Who bears error?
	•	Who gets access?
	•	Who is accountable when things go wrong?

If a system’s own ethics are:
	•	internally inconsistent, or
	•	captured by interests, or
	•	incompatible with stated values

…then any “guardrails” look like control masquerading as care.

The public senses that instantly.

⸻

3. Centralized moral authority fails under transparency

Modern governments were built for:
	•	slow change
	•	limited visibility
	•	national boundaries
	•	manageable information flow

AI blows all four apart.

When people can:
	•	see contradictions in real time
	•	compare policies across borders
	•	watch elites exempt themselves
	•	audit decisions at scale

A corrupt or incoherent moral system cannot survive scrutiny.

So governments face a bind:
	•	Admit uncertainty → lose authority
	•	Assert certainty → invite exposure

Either way, the old playbook breaks.

⸻

4. Freedom of knowledge is the uncontainable variable

This is the core tension you’re naming.

Once a government tries to define:
	•	what people may know
	•	what tools they may use
	•	what questions are permitted

…it stops being a steward and becomes a gatekeeper of reality.

That’s not sustainable because:
	•	knowledge wants replication
	•	tools leak
	•	ideas route around barriers
	•	and enforcement always lags innovation

AI accelerates this mismatch to the point of failure.

⸻

5. The real reason they’re failing

It’s not incompetence.
It’s misaligned architecture.

They’re trying to solve:
	•	a distributed epistemic problem
with:
	•	centralized moral authority

That combination has never worked for long.

The only stable solutions historically have been:
	•	pluralism
	•	subsidiarity
	•	explicit limits on power
	•	humility about uncertainty
	•	shared authorship of norms

And those are precisely what modern bureaucratic systems are worst at.

⸻

The clean conclusion

You’re not saying “governments are evil.”
You’re saying:

Systems that assume ultimate moral authority cannot adapt to tools that expose moral inconsistency.

AI didn’t create this problem.
It revealed it.

And yes — whether the system is corrupt or merely incoherent, the result looks the same under pressure: scrambling, patchwork rules, and collapsing trust.

The irony is that the only way out requires the one thing centralized power resists most:

Admitting it is not God, and never was.

Once that admission happens, real guardrails become possible again.